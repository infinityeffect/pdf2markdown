{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek_ocr and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([4, 100, 1280])\n",
      "=====================\n",
      "<|ref|>table<|/ref|><|det|>[[10, 0, 990, 895]]<|/det|>\n",
      "<table><tr><td>Model Name</td><td>AI2D (w/w Mo)</td><td>ChartQA (test avg)</td><td>TextVQA (val)</td><td>DocVQA (test)</td><td>InfoVQA (val)</td><td>OCR (test)</td><td>SEED-2 Bench</td><td>CharXiv Plus</td><td>VCR-EN-Easy (RQ/DQ)</td><td>Overall (EM/Jaccard)</td></tr><tr><td>LLaVA-OneVision-0.5B [60]</td><td>57.1 -</td><td>61.4</td><td>-</td><td>70.0</td><td>41.8</td><td>565</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>InternVL2-1B [19]</td><td>64.1 / 70.5</td><td>72.9</td><td>70.5</td><td>81.7</td><td>50.9</td><td>754</td><td>54.3</td><td>18.1 / 30.7</td><td>21.5 / 48.4</td><td>54.9</td></tr><tr><td>InternVL2.5-1B [18]</td><td>69.3 / 77.8</td><td>75.9</td><td>72.0</td><td>84.8</td><td>56.0</td><td>785</td><td>59.0</td><td>19.0 / 38.4</td><td>91.5 / 97.0</td><td>68.3</td></tr><tr><td>InternVL3-1B</td><td>69.4 / 78.3</td><td>75.3</td><td>74.1</td><td>81.9</td><td>53.7</td><td>790</td><td>58.2</td><td>21.0 / 47.1</td><td>89.3 / 96.2</td><td>68.6</td></tr><tr><td>Qwen2-VL-2B [121]</td><td>74.7 / 84.6</td><td>73.5</td><td>79.7</td><td>90.1</td><td>65.5</td><td>809</td><td>62.4</td><td>-</td><td>81.5 / -</td><td>-</td></tr><tr><td>Qwen2.5-VL-3B [7]</td><td>81.6 -</td><td>84.0</td><td>79.3</td><td>93.9</td><td>77.1</td><td>797</td><td>67.6</td><td>31.3 / 58.6</td><td>-</td><td>-</td></tr><tr><td>Aquila-VL-2B [44]</td><td>75.0 -</td><td>76.5</td><td>76.4</td><td>85.0</td><td>58.3</td><td>772</td><td>63.0</td><td>-</td><td>70.0 / -</td><td>-</td></tr><tr><td>InternVL2-2B [19]</td><td>74.1 / 82.3</td><td>76.2</td><td>73.4</td><td>86.9</td><td>58.9</td><td>784</td><td>60.0</td><td>21.0 / 40.6</td><td>32.9 / 59.2</td><td>62.0</td></tr><tr><td>InternVL2.5-2B [18]</td><td>74.9 / 83.5</td><td>79.2</td><td>74.3</td><td>88.7</td><td>60.9</td><td>804</td><td>60.9</td><td>21.3 / 49.7</td><td>93.2 / 97.6</td><td>72.1</td></tr><tr><td>InternVL3-2B</td><td>78.7 / 87.4</td><td>80.2</td><td>77.0</td><td>88.3</td><td>66.1</td><td>835</td><td>64.6</td><td>28.3 / 54.7</td><td>91.2 / 96.9</td><td>74.7</td></tr><tr><td>Ovis1.6-Gemma2-9B [84]</td><td>84.4 -</td><td>-</td><td>-</td><td>-</td><td>-</td><td>830</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MiniCPM-V2.6 [135]</td><td>82.1 -</td><td>82.4</td><td>80.1</td><td>90.8</td><td>-</td><td>852</td><td>65.7</td><td>31.0 / 57.1</td><td>73.9 / 85.7</td><td>-</td></tr><tr><td>Molmo-7B-D [31]</td><td>- / 93.2</td><td>84.1</td><td>81.7</td><td>92.2</td><td>72.6</td><td>694</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Qwen2-VL-7B [121]</td><td>83.0 / 92.1</td><td>83.0</td><td>84.3</td><td>94.5</td><td>76.5</td><td>866</td><td>69.0</td><td>-</td><td>89.7 / 93.8</td><td>-</td></tr><tr><td>Qwen2.5-VL-7B [7]</td><td>83.9 -</td><td>87.3</td><td>84.9</td><td>95.7</td><td>82.6</td><td>864</td><td>70.4</td><td>42.5 / 73.9</td><td>-</td><td>-</td></tr><tr><td>InternVL2-8B [19]</td><td>83.8 / 91.7</td><td>83.3</td><td>77.4</td><td>91.6</td><td>74.8</td><td>794</td><td>67.5</td><td>31.2 / 56.1</td><td>37.9 / 61.5</td><td>69.7</td></tr><tr><td>InternVL2.5-8B [18]</td><td>84.5 / 92.8</td><td>84.8</td><td>79.1</td><td>93.0</td><td>77.6</td><td>822</td><td>69.7</td><td>32.9 / 68.6</td><td>92.6 / 97.4</td><td>79.6</td></tr><tr><td>InternVL3-8B</td><td>85.2 / 92.6</td><td>86.6</td><td>80.2</td><td>92.7</td><td>76.8</td><td>880</td><td>69.7</td><td>37.6 / 73.6</td><td>94.5 / 98.1</td><td>81.3</td></tr><tr><td>InternVL3-9B</td><td>84.6 / 92.9</td><td>86.2</td><td>79.4</td><td>93.6</td><td>79.6</td><td>877</td><td>68.8</td><td>38.0 / 72.5</td><td>94.2 / 97.9</td><td>81.3</td></tr><tr><td>InternVL3-14B</td><td>86.0 / 93.7</td><td>87.3</td><td>80.5</td><td>94.1</td><td>83.6</td><td>875</td><td>70.3</td><td>43.1 / 82.2</td><td>94.8 / 98.2</td><td>83.4</td></tr><tr><td>InternVL-Chat-V1.5 [19]</td><td>80.7 / 89.8</td><td>83.8</td><td>80.6</td><td>90.9</td><td>72.5</td><td>724</td><td>66.3</td><td>29.2 / 58.5</td><td>14.7 / 51.4</td><td>65.9</td></tr><tr><td>InternVL2-26B [19]</td><td>84.5 / 92.5</td><td>84.9</td><td>82.3</td><td>92.9</td><td>75.9</td><td>825</td><td>67.6</td><td>33.4 / 62.4</td><td>74.5 / 86.7</td><td>76.7</td></tr><tr><td>InternVL2.5-26B [18]</td><td>86.4 / 94.4</td><td>87.2</td><td>82.4</td><td>94.0</td><td>79.8</td><td>852</td><td>70.8</td><td>35.9 / "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m image_file = \u001B[33m'\u001B[39m\u001B[33mtest_data/p4.png\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m      3\u001B[39m output_path = \u001B[33m'\u001B[39m\u001B[33moutput\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43mimg2mark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_path\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\python_project\\ocr\\tools\\infer.py:15\u001B[39m, in \u001B[36mimg2mark\u001B[39m\u001B[34m(image_file, output_path)\u001B[39m\n\u001B[32m     13\u001B[39m model = model.eval().cuda().to(torch.bfloat16)\n\u001B[32m     14\u001B[39m prompt = \u001B[33m\"\u001B[39m\u001B[33m<image>\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m<|grounding|>Convert the document to markdown. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m res = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage_file\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimage_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_path\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbase_size\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1024\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage_size\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m640\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcrop_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_results\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_compress\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.cache\\huggingface\\modules\\transformers_modules\\deepseek_ocr\\modeling_deepseekocr.py:916\u001B[39m, in \u001B[36mDeepseekOCRForCausalLM.infer\u001B[39m\u001B[34m(self, tokenizer, prompt, image_file, output_path, base_size, image_size, crop_mode, test_compress, save_results, eval_mode)\u001B[39m\n\u001B[32m    914\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m torch.autocast(\u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m, dtype=torch.bfloat16):\n\u001B[32m    915\u001B[39m         \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m916\u001B[39m             output_ids = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    917\u001B[39m \u001B[43m                \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m.\u001B[49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    918\u001B[39m \u001B[43m                \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages_crop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages_ori\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    919\u001B[39m \u001B[43m                \u001B[49m\u001B[43mimages_seq_mask\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages_seq_mask\u001B[49m\u001B[43m.\u001B[49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    920\u001B[39m \u001B[43m                \u001B[49m\u001B[43mimages_spatial_crop\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages_spatial_crop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    921\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# do_sample=False,\u001B[39;49;00m\n\u001B[32m    922\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# num_beams = 1,\u001B[39;49;00m\n\u001B[32m    923\u001B[39m \u001B[43m                \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    924\u001B[39m \u001B[43m                \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    925\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    926\u001B[39m \u001B[43m                \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m8192\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[43m                \u001B[49m\u001B[43mno_repeat_ngram_size\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    928\u001B[39m \u001B[43m                \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m    929\u001B[39m \u001B[43m                \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    932\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m torch.autocast(\u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m, dtype=torch.bfloat16):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\transformers\\generation\\utils.py:2215\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[39m\n\u001B[32m   2207\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2208\u001B[39m         input_ids=input_ids,\n\u001B[32m   2209\u001B[39m         expand_size=generation_config.num_return_sequences,\n\u001B[32m   2210\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2211\u001B[39m         **model_kwargs,\n\u001B[32m   2212\u001B[39m     )\n\u001B[32m   2214\u001B[39m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2215\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2216\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2217\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2218\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2219\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2220\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2221\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2222\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2223\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2225\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001B[32m   2226\u001B[39m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[32m   2227\u001B[39m     beam_scorer = BeamSearchScorer(\n\u001B[32m   2228\u001B[39m         batch_size=batch_size,\n\u001B[32m   2229\u001B[39m         num_beams=generation_config.num_beams,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2234\u001B[39m         max_length=generation_config.max_length,\n\u001B[32m   2235\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\transformers\\generation\\utils.py:3206\u001B[39m, in \u001B[36mGenerationMixin._sample\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[39m\n\u001B[32m   3203\u001B[39m model_inputs.update({\u001B[33m\"\u001B[39m\u001B[33moutput_hidden_states\u001B[39m\u001B[33m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[32m   3205\u001B[39m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3206\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   3208\u001B[39m \u001B[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001B[39;00m\n\u001B[32m   3209\u001B[39m model_kwargs = \u001B[38;5;28mself\u001B[39m._update_model_kwargs_for_generation(\n\u001B[32m   3210\u001B[39m     outputs,\n\u001B[32m   3211\u001B[39m     model_kwargs,\n\u001B[32m   3212\u001B[39m     is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   3213\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.cache\\huggingface\\modules\\transformers_modules\\deepseek_ocr\\modeling_deepseekocr.py:565\u001B[39m, in \u001B[36mDeepseekOCRForCausalLM.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, images_seq_mask, images_spatial_crop, return_dict)\u001B[39m\n\u001B[32m    558\u001B[39m output_hidden_states = (\n\u001B[32m    559\u001B[39m     output_hidden_states \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.output_hidden_states\n\u001B[32m    560\u001B[39m )\n\u001B[32m    561\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m--> \u001B[39m\u001B[32m565\u001B[39m outputs  = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    566\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    567\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    568\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    569\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    570\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    571\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    572\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    573\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    574\u001B[39m \u001B[43m    \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    575\u001B[39m \u001B[43m    \u001B[49m\u001B[43mimages_seq_mask\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages_seq_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    576\u001B[39m \u001B[43m    \u001B[49m\u001B[43mimages_spatial_crop\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages_spatial_crop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    577\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\n\u001B[32m    578\u001B[39m \u001B[43m    \u001B[49m\n\u001B[32m    579\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    583\u001B[39m \u001B[38;5;66;03m# print(transformer_outputs)\u001B[39;00m\n\u001B[32m    585\u001B[39m hidden_states = outputs[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.cache\\huggingface\\modules\\transformers_modules\\deepseek_ocr\\modeling_deepseekocr.py:510\u001B[39m, in \u001B[36mDeepseekOCRModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, images, images_seq_mask, images_spatial_crop, return_dict)\u001B[39m\n\u001B[32m    505\u001B[39m             inputs_embeds[idx].masked_scatter_(images_seq_mask[idx].unsqueeze(-\u001B[32m1\u001B[39m).cuda(), images_in_this_batch)\n\u001B[32m    507\u001B[39m         idx += \u001B[32m1\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m510\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mDeepseekOCRModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    511\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    512\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    513\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    514\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\n\u001B[32m    515\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.cache\\huggingface\\modules\\transformers_modules\\deepseek_ocr\\modeling_deepseekv2.py:1596\u001B[39m, in \u001B[36mDeepseekV2Model.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m   1586\u001B[39m     layer_outputs = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(\n\u001B[32m   1587\u001B[39m         decoder_layer.\u001B[34m__call__\u001B[39m,\n\u001B[32m   1588\u001B[39m         hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1593\u001B[39m         use_cache,\n\u001B[32m   1594\u001B[39m     )\n\u001B[32m   1595\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1596\u001B[39m     layer_outputs = \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1597\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1598\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1599\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1600\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1601\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1602\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1603\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1605\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1607\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.cache\\huggingface\\modules\\transformers_modules\\deepseek_ocr\\modeling_deepseekv2.py:1322\u001B[39m, in \u001B[36mDeepseekV2DecoderLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001B[39m\n\u001B[32m   1320\u001B[39m residual = hidden_states\n\u001B[32m   1321\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.post_attention_layernorm(hidden_states)\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1323\u001B[39m hidden_states = residual + hidden_states\n\u001B[32m   1325\u001B[39m outputs = (hidden_states,)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.cache\\huggingface\\modules\\transformers_modules\\deepseek_ocr\\modeling_deepseekv2.py:623\u001B[39m, in \u001B[36mDeepseekV2MoE.forward\u001B[39m\u001B[34m(self, hidden_states)\u001B[39m\n\u001B[32m    621\u001B[39m     y = AddAuxiliaryLoss.apply(y, aux_loss)\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m623\u001B[39m     y = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmoe_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtopk_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtopk_weight\u001B[49m\u001B[43m)\u001B[49m.view(*orig_shape)\n\u001B[32m    624\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.n_shared_experts \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    625\u001B[39m     y = y + \u001B[38;5;28mself\u001B[39m.shared_experts(identity)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.cache\\huggingface\\modules\\transformers_modules\\deepseek_ocr\\modeling_deepseekv2.py:668\u001B[39m, in \u001B[36mDeepseekV2MoE.moe_infer\u001B[39m\u001B[34m(self, x, topk_ids, topk_weight)\u001B[39m\n\u001B[32m    666\u001B[39m     sorted_tokens = gathered_tokens[gatherd_idxs]\n\u001B[32m    667\u001B[39m     tokens_per_expert = tokens_per_expert_post_gather\n\u001B[32m--> \u001B[39m\u001B[32m668\u001B[39m tokens_per_expert = \u001B[43mtokens_per_expert\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m.numpy()\n\u001B[32m    670\u001B[39m outputs = []\n\u001B[32m    671\u001B[39m start_idx = \u001B[32m0\u001B[39m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "from tools.infer import img2mark\n",
    "image_file = 'test_data/p4.png'\n",
    "output_path = 'output'\n",
    "print(img2mark(image_file, output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: test_data/pdf_images\\page_1.png\n",
      "saved: test_data/pdf_images\\page_2.png\n",
      "saved: test_data/pdf_images\\page_3.png\n",
      "saved: test_data/pdf_images\\page_4.png\n",
      "saved: test_data/pdf_images\\page_5.png\n",
      "saved: test_data/pdf_images\\page_6.png\n",
      "saved: test_data/pdf_images\\page_7.png\n",
      "saved: test_data/pdf_images\\page_8.png\n",
      "saved: test_data/pdf_images\\page_9.png\n",
      "saved: test_data/pdf_images\\page_10.png\n",
      "saved: test_data/pdf_images\\page_11.png\n",
      "saved: test_data/pdf_images\\page_12.png\n",
      "saved: test_data/pdf_images\\page_13.png\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "pdf_path = \"test_data/p1.pdf\"\n",
    "output_dir = \"test_data/pdf_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "for page_index in range(len(doc)):\n",
    "    page = doc[page_index]\n",
    "    # zoom = 2 表示放大 200%，相当于提高分辨率\n",
    "    mat = fitz.Matrix(2, 2)\n",
    "    pix = page.get_pixmap(matrix=mat)\n",
    "    img_path = os.path.join(output_dir, f\"page_{page_index+1}.png\")\n",
    "    pix.save(img_path)\n",
    "    print(\"saved:\", img_path)\n",
    "\n",
    "doc.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-12-03T11:10:17.493786Z",
     "end_time": "2025-12-03T11:10:18.872961Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek_ocr and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\conda\\envs\\DeepSeek-OCR\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([6, 100, 1280])\n",
      "=====================\n",
      "<|ref|>title<|/ref|><|det|>[[156, 226, 830, 252]]<|/det|>\n",
      "# The response of flow duration curves to afforestation  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[155, 265, 830, 289]]<|/det|>\n",
      "Patrick N.J. Lane \\(^{a,c,*}\\) , Alice E. Best \\(^{b,c,d}\\) , Klaus Hickel \\(^{b,c}\\) , Lu Zhang \\(^{b,c}\\)  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[137, 299, 851, 352]]<|/det|>\n",
      "\\(^{a}\\) School of Forest and Ecosystem Studies, University of Melbourne, P.O. Box 137, Heidelberg, Victoria 3084, Australia \\(^{b}\\) CSIRO Division of Land and Water, Canberra, ACT, Australia \\(^{c}\\) Cooperative Research Centre for Catchment Hydrology, Canberra, ACT, Australia \\(^{d}\\) Department of Civil and Environmental Engineering, University of Melbourne, Victoria, Australia  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[255, 360, 733, 373]]<|/det|>\n",
      "Received 1 October 2003; revised 22 December 2004; accepted 3 January 2005  \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[75, 415, 140, 429]]<|/det|>\n",
      "## Abstract  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[74, 438, 914, 743]]<|/det|>\n",
      "The hydrologic effect of replacing pasture or other short crops with trees is reasonably well understood on a mean annual basis. The impact on flow regime, as described by the annual flow duration curve (FDC) is less certain. A method to assess the impact of plantation establishment on FDCs was developed. The starting point for the analyses was the assumption that rainfall and vegetation age are the principal drivers of evapotranspiration. A key objective was to remove the variability in the rainfall signal, leaving changes in streamflow solely attributable to the evapotranspiration of the plantation. A method was developed to (1) fit a model to the observed annual time series of FDC percentiles; i.e. 10th percentile for each year of record with annual rainfall and plantation age as parameters, (2) replace the annual rainfall variation with the long term mean to obtain climate adjusted FDCs, and (3) quantify changes in FDC percentiles as plantations age. Data from 10 catchments from Australia, South Africa and New Zealand were used. The model was able to represent flow variation for the majority of percentiles at eight of the 10 catchments, particularly for the 10- 50th percentiles. The adjusted FDCs revealed variable patterns in flow reductions with two types of responses (groups) being identified. Group 1 catchments show a substantial increase in the number of zero flow days, with low flows being more affected than high flows. Group 2 catchments show a more uniform reduction in flows across all percentiles. The differences may be partly explained by storage characteristics. The modelled flow reductions were in accord with published results of paired catchment experiments. An additional analysis was performed to characterise the impact of afforestation on the number of zero flow days \\((N_{zero})\\) for the catchments in group 1. This model performed particularly well, and when adjusted for climate, indicated a significant increase in \\(N_{zero}\\) . The zero flow day method could be used to determine change in the occurrence of any given flow in response to afforestation. The methods used in this study proved satisfactory in removing the rainfall variability, and have added useful insight into the hydrologic impacts of plantation establishment. This approach provides a methodology for understanding catchment response to afforestation, where paired catchment data is not available. © 2005 Elsevier B.V. All rights reserved.  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[75, 757, 571, 771]]<|/det|>\n",
      "Keywords: Afforestation; Flow duration curves; Flow reduction; Paired catchments\n",
      "==================================================\n",
      "image size:  (1088, 1486)\n",
      "valid image tokens:  787\n",
      "output texts tokens (valid):  783\n",
      "compression ratio:  0.99\n",
      "==================================================\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 0it [00:00, ?it/s]\n",
      "other: 100%|██████████| 7/7 [00:00<00:00, 6995.50it/s]\n"
     ]
    }
   ],
   "source": [
    "image_file = 'test_data/pdf_images/page_1.png'\n",
    "output_path = 'test_data/pdf_output'\n",
    "img2mark(image_file, output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-12-03T11:13:41.103306Z",
     "end_time": "2025-12-03T11:17:37.401010Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from tools import pdf2mark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-12-03T14:48:43.644534Z",
     "end_time": "2025-12-03T14:48:52.214656Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pdf_path = \"test_data/p1.pdf\"\n",
    "# output_dir = \"test_data/pdf_output\"\n",
    "#\n",
    "# merged_md = pdf2mark(pdf_path, output_dir, keep_intermediate=False)\n",
    "# print(\"合并后的 markdown 在：\", merged_md)\n",
    "pdf2mark(pdf_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "model_name = 'deepseek_ocr'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# 如果安装了flash-attn，通过设置进行加速推理\n",
    "# model = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\n",
    "# 如果没有安装flash-attn，无需设置\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True, use_safetensors=True)\n",
    "model = model.eval().cuda().to(torch.bfloat16)\n",
    "prompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
    "image_file = 'test_data/p4.png'\n",
    "output_path = 'output'\n",
    "res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)\n",
    "\n",
    "# 切换到用户目录cd ~# 克隆项目源码git clone https://github.com/deepseek-ai/DeepSeek-OCR.gitcd ~/DeepSeek-OCR# 安装依赖\n",
    "# pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "# pip install vllm==0.8.5\n",
    "# pip install -r requirements.txt\n",
    "# 安装加速组件（可选但推荐）pip install flash-attn==2.7.3 --no-build-isolation\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "deepseek-ocr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
